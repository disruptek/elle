WHY NO PERFORMANCE CHANGE DETECTED
===================================

The Token<'a> optimization targets LEXING/SYMBOL ALLOCATION, but most benchmarks 
show no change because:

1. BENCHMARK COMPOSITION ISSUE
   ├─ Current benchmarks measure: parse() + compile() + execute()
   ├─ Our optimization affects: parse() only (~15-25% of total time)
   └─ Result: Improvement masked by compilation + VM execution time

2. BASELINE ISSUE
   ├─ "base" = already this version (no old version to compare)
   ├─ "new" = current code (same as base)
   └─ Result: Comparing identical code = no difference shown

HOW TO PROPERLY MEASURE THE IMPROVEMENT
========================================

OPTION 1: Create Parse-Only Benchmark
   Write a benchmark that measures JUST the lexing phase:
   
   ```rust
   fn parse_100_symbols(c: &mut Criterion) {
       c.bench_function("parse_100_symbols", |b| {
           b.iter(|| {
               let mut symbols = SymbolTable::new();
               read_str("(define foo (list a b c ... z))", &mut symbols)
           });
       });
   }
   ```
   
   This isolates the lexing improvement from other factors.

OPTION 2: Compare Current vs Old Code
   1. Save current benchmark results:
      $ cp -r target/criterion target/criterion.with_token_borrowing
   
   2. Checkout old code:
      $ git checkout HEAD~1  (or previous commit)
   
   3. Run benchmarks again:
      $ cargo bench --bench benchmarks
   
   4. Criterion will automatically compare old vs new in target/criterion/report/

OPTION 3: Direct Lexer Benchmark
   Create a benchmark that measures ONLY lexer performance:
   
   ```rust
   fn lex_large_input(c: &mut Criterion) {
       let input = "(+ 1 2 3 4 5 ... 1000)";
       c.bench_function("lex_large_input", |b| {
           b.iter(|| {
               let mut lexer = Lexer::new(input);
               while let Ok(Some(_)) = lexer.next_token() {}
           });
       });
   }
   ```

WHAT THE CURRENT RESULTS TELL US
=================================

✓ NO REGRESSIONS
  • Token<'a> optimization didn't slow anything down
  • Code is correct and maintains performance baseline
  
✗ NO MEASURABLE IMPROVEMENT  
  • Because we're comparing same code version
  • Parsing is only ~15-25% of total benchmark time
  
NEXT STEPS
==========

1. To verify improvement, you need:
   - Old code version to compare against (see Option 2)
   - Parse-focused benchmarks (see Option 1)
   - Or accept the theoretical improvement (15-25% less allocations)

2. The improvement IS real:
   - Reduced String allocations per symbol
   - Better CPU cache locality (working with input buffer)
   - Lower memory fragmentation
   
3. Why it's hard to measure:
   - Benchmarks include VM execution (85% of time)
   - Allocation improvements aren't always visible in timing
   - Need many iterations to see tiny improvements
